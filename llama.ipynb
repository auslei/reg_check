{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5250e250-c277-422f-b493-d7985793185e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps is found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rp/svy321qd3lv_hlkfndpyd3hr0000gn/T/ipykernel_20581/314438879.py:14: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  if torch.has_mps:\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "# Determine which divice to use.\n",
    "def determine_device():\n",
    "    device = \"cpu\"\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "\n",
    "    if torch.backends.mps.is_built():\n",
    "        device = \"mps\"\n",
    "    \n",
    "    print(f\"{device} is found.\") \n",
    "\n",
    "    return device\n",
    "\n",
    "device = determine_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87571a8-f451-45b5-b5ba-b23db5f82d45",
   "metadata": {},
   "source": [
    "# Load Model from HuggingFace\n",
    "\n",
    "Reduce the memory footprint using Quantization BitsAndBytesConfig.\n",
    "https://huggingface.co/docs/transformers/main_classes/quantization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07a851d0-e5a7-4cf4-9db6-8cfa879053a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "# begin initializing HF items, you need an access token\n",
    "hf_auth = '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'\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af6125f-133c-4d98-8b49-c81ffa1b82ca",
   "metadata": {},
   "source": [
    "## Load the Model\n",
    "This takes a while ~10 - 15 minutes. Grab a coffee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd7b4d7b-69d6-417f-a01a-979a99480880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "CUDA SETUP: Loading binary /opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n",
      "dlopen(/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so, 0x0006): tried: '/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (not a mach-o file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (no such file), '/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (not a mach-o file)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb6dc60525c84a068f1104f7bcb155ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on mps\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    use_auth_token=hf_auth,\n",
    "    offload_folder = \"./offload\"\n",
    ")\n",
    "\n",
    "# enable evaluation mode to allow model inference\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa34b6fc-c5e5-45b4-857e-70864dabf480",
   "metadata": {},
   "source": [
    "## Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63efe144-e6bd-42c5-b844-be6b2bab18cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 65.8 ms, sys: 19 ms, total: 84.8 ms\n",
      "Wall time: 340 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e2505c-8733-4e58-854e-5a98ea06a880",
   "metadata": {},
   "source": [
    "## Create a pipeline object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b68b57e-cc4c-4686-9296-7db8da619031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([    1, 29871,    13, 29950,  7889, 29901], device='mps:0'),\n",
       " tensor([    1, 29871,    13, 28956,    13], device='mps:0')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# where the model will stop generating tokens\n",
    "print(device)\n",
    "stop_list = ['\\nHuman:', '\\n```\\n']\n",
    "\n",
    "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
    "stop_token_ids\n",
    "\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
    "stop_token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ff9032-e4c5-4e7f-bb81-67ae4417e0d9",
   "metadata": {},
   "source": [
    "### Defining stopping criteria\n",
    "\n",
    "https://huggingface.co/docs/transformers/v4.31.0/en/internal/generation_utils#transformers.StoppingCriteria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eba69a6-3314-4c5b-8528-a45f03adb52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# define custom stopping criteria object\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5e2385-5785-41a9-84c6-2706b7ba824a",
   "metadata": {},
   "source": [
    "### Defining the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38f31d59-8717-4026-aff3-ac16ddaed656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.0.1 with CUDA None (you have 2.1.0.dev20230806)\n",
      "    Python  3.11.3 (you have 3.11.3)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    }
   ],
   "source": [
    "llm = transformers.pipeline(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
    "    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  # max number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aea60b-d2e8-45d2-b6a2-3ce591c79503",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = llm(\"Explain me the difference between Data Lakehouse and Data Warehouse.\")\n",
    "print(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59889de3-46ad-4202-9180-9a215ddffd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "llm = HuggingFaceHub(repo_id='google/flan-t5-xl', model_kwargs={'temperature':1e-10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c047a9b-d131-4628-927b-2172a8b981b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(\"what is the capital of China?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cda329-724d-4c96-8e14-242993fe9ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
